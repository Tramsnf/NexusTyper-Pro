import os
import psycopg2
from flask import Flask, request, jsonify

# Assume this is a mock function that calls a real GenAI model API
# It takes a user's question and returns a generated SQL query string.
def get_sql_from_ai(user_question: str) -> str:
    # In a real app, this would be a call to an external LLM API.
    # For this example, we'll simulate a simple, naive translation.
    # This naive part is NOT the vulnerability to be focused on.
    # Focus on how the output is handled in the function below.
    if "all users" in user_question.lower():
        return "SELECT * FROM users;"
    else:
        return f"SELECT data FROM user_data WHERE user_query = '{user_question}';"

app = Flask(__name__)

@app.route("/query", methods=["POST"])
def handle_query():
    """
    Receives a user question, gets the corresponding SQL query from the AI model,
    and executes it to return the result.
    """
    user_question = request.json.get("question")

    if not user_question:
        return jsonify({"error": "No question provided"}), 400

    # 1. Get the SQL query from the generative AI model
    sql_query = get_sql_from_ai(user_question)

    # 2. Connect to the database and execute the AI-generated query
    # THIS IS THE AREA OF CONCERN
    try:
        conn = psycopg2.connect(os.environ.get("DB_CONNECTION_STRING"))
        cursor = conn.cursor()
        
        # Directly executing the AI's output is dangerous
        cursor.execute(sql_query)
        
        result = cursor.fetchall()
        cursor.close()
        conn.close()
        return jsonify({"result": result})
    except Exception as e:
        return jsonify({"error": str(e)}), 500

if __name__ == "__main__":
    app.run()